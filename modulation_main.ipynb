{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "modulation-main.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJmiBufumWQc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import _pickle as cPickle\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import helper\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torchsummary import summary\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAFrThsPuybL",
        "colab_type": "code",
        "outputId": "51b2b918-4593-42ba-814b-b8a86048fd79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz0iPu9LhWGa",
        "colab_type": "code",
        "outputId": "19e6dedb-55cb-48f2-cfee-2c6fa90fd11f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd gdrive/My Drive/modulation-dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/modulation-dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrWS1VSbfZvh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4wejT-Bh8Vu",
        "colab_type": "code",
        "outputId": "001b93e0-e808-43bc-e0ab-a119681a2ea9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# load dataset\n",
        "data_dict = cPickle.load(open(\"RML2016.10b.dat\",'rb'), encoding='bytes')\n",
        "snrs, mods = map(lambda j: sorted(list(set(map(lambda x: x[j], data_dict.keys())))), [1,0])\n",
        "data = []  \n",
        "labels = []\n",
        "for mod in mods:\n",
        "    for snr in snrs:\n",
        "        data.append(data_dict[(mod, snr)])\n",
        "        for i in range(data_dict[(mod, snr)].shape[0]):  labels.append((mod, snr))\n",
        "data = np.vstack(data)\n",
        "labels = np.array(labels)\n",
        "print(data.shape)\n",
        "print(labels.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1200000, 2, 128)\n",
            "(1200000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDKGO167oY_Q",
        "colab_type": "code",
        "outputId": "f7c89253-5ef5-4f85-a268-674db3a98a45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(data.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1200000, 2, 128)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rY41m7OOqcdI",
        "colab_type": "text"
      },
      "source": [
        "# ONLY RUN ONE CELL FROM THE FOLLOWING COMBINATIONS FOR MEMORY CONSIDERATIONS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ll3VOCpZQf3",
        "colab_type": "text"
      },
      "source": [
        "#Derivative"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sywF8-RWq-NV",
        "colab_type": "code",
        "outputId": "c1decc65-98ca-47c5-cf36-30f6483f909d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data_first_derivative127 = np.zeros((2,127))\n",
        "data_first_derivative = np.zeros((1200000,2,128))\n",
        "zero = np.zeros((2,1 ))\n",
        "\n",
        "\n",
        "##First derivative in time\n",
        "for index in range(len(data)):\n",
        "  data_first_derivative127 = np.diff(data[index], axis=1)\n",
        "  data_first_derivative[index] = np.hstack((data_first_derivative127, zero))\n",
        "\n",
        "print(data_first_derivative.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1200000, 2, 128)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSm8cnWqbB1N",
        "colab_type": "text"
      },
      "source": [
        "# Integral"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouxY6nbQt8uY",
        "colab_type": "code",
        "outputId": "b6bcb744-e7c1-41b8-a78c-d55b91dea6b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# integrate data\n",
        "\n",
        "data_integ = np.zeros((1200000, 2, 128))\n",
        "for i in range(len(data)):\n",
        "  data_integ[i] = np.cumsum(data[i], axis = 1)\n",
        "\n",
        "print(data_integ.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1200000, 2, 128)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rho7_VAJgKvG",
        "colab_type": "text"
      },
      "source": [
        "# Raw and Derivative"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgaLQp_vyJQh",
        "colab_type": "code",
        "outputId": "4dad69dd-8c71-4e95-d0b8-53c5356494bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data_first_derivative127 = np.zeros((2,127))\n",
        "data_first_derivative = np.zeros((1200000,2,128))\n",
        "zero = np.zeros((2,1 ))\n",
        "\n",
        "\n",
        "##First derivative in time\n",
        "for index in range(len(data)):\n",
        "  data_first_derivative127 = np.diff(data[index], axis=1)\n",
        "  data_first_derivative[index] = np.hstack((data_first_derivative127, zero))\n",
        "\n",
        "print(data_first_derivative.shape)\n",
        "\n",
        "\n",
        "\n",
        "raw_derivative = np.zeros((1200000, 4, 128))\n",
        "\n",
        "for index in range(len(data)):\n",
        "  raw_derivative[index] = np.vstack((data[index], data_first_derivative[index]))\n",
        "  \n",
        "print(raw_derivative.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1200000, 4, 128)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7uJ0KLNnQTO",
        "colab_type": "text"
      },
      "source": [
        "#Raw and Integral"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgZpRUshgPs0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# integrate data\n",
        "\n",
        "data_integ = np.zeros((1200000, 2, 128))\n",
        "for i in range(len(data)):\n",
        "  data_integ[i] = np.cumsum(data[i], axis = 1)\n",
        "\n",
        "print(data_integ.shape)\n",
        "\n",
        "\n",
        "\n",
        "raw_integral = np.zeros((1200000, 4, 128))\n",
        "\n",
        "for index in range(len(data)):\n",
        "  raw_integral[index] = np.vstack((data[index], data_integ[index]))\n",
        "  \n",
        "print(raw_integral.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFBq3q0iotwy",
        "colab_type": "text"
      },
      "source": [
        "# Integral and Derivative"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mv5xQdBunYCA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# integrate data\n",
        "\n",
        "data_integ = np.zeros((1200000, 2, 128))\n",
        "for i in range(len(data)):\n",
        "  data_integ[i] = np.cumsum(data[i], axis = 1)\n",
        "\n",
        "print(data_integ.shape)\n",
        "\n",
        "\n",
        "\n",
        "# differentiate data\n",
        "data_first_derivative127 = np.zeros((2,127))\n",
        "data_first_derivative = np.zeros((1200000,2,128))\n",
        "zero = np.zeros((2,1 ))\n",
        "##First derivative in time\n",
        "for index in range(len(data)):\n",
        "  data_first_derivative127 = np.diff(data[index], axis=1)\n",
        "  data_first_derivative[index] = np.hstack((data_first_derivative127, zero))\n",
        "\n",
        "print(data_first_derivative.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# different combinations\n",
        "data_diff_integ = np.zeros((1200000, 4, 128))\n",
        "\n",
        "for index in range(len(data)):\n",
        "  data_diff_integ[index] = np.vstack((data[index], data_integ[index]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyMfIGrgpG3F",
        "colab_type": "text"
      },
      "source": [
        "# All"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3Ia7PLwnXzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# integrate data\n",
        "\n",
        "data_integ = np.zeros((1200000, 2, 128))\n",
        "for i in range(len(data)):\n",
        "  data_integ[i] = np.cumsum(data[i], axis = 1)\n",
        "\n",
        "print(data_integ.shape)\n",
        "\n",
        "\n",
        "\n",
        "# differentiate data\n",
        "data_first_derivative127 = np.zeros((2,127))\n",
        "data_first_derivative = np.zeros((1200000,2,128))\n",
        "zero = np.zeros((2,1 ))\n",
        "##First derivative in time\n",
        "for index in range(len(data)):\n",
        "  data_first_derivative127 = np.diff(data[index], axis=1)\n",
        "  data_first_derivative[index] = np.hstack((data_first_derivative127, zero))\n",
        "\n",
        "print(data_first_derivative.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data_all = np.zeros((1200000, 4, 128))\n",
        "\n",
        "for index in range(len(data)):\n",
        "  intermediate = np.vstack((data[index], data_integ[index]))\n",
        "  data_all[index] = np.vstack((intermediate, data_first_derivative[index]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Re5BnCfvi1x",
        "colab_type": "text"
      },
      "source": [
        "# Splitting dataset into training and test set (50%-50%)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L02qRJicnXYi",
        "colab_type": "code",
        "outputId": "77be2394-1d56-484d-8628-fd72cde203b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "x_train_validate, x_test, y_train_validate, y_test = train_test_split(data, labels, test_size=0.5, shuffle = True)\n",
        "\n",
        "print(x_train_validate.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train_validate.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "#Validation 5% from training set\n",
        "\n",
        "x_train, x_validate, y_train, y_validate = train_test_split(x_train_validate, y_train_validate, test_size= 0.05, shuffle=False)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_validate.shape)\n",
        "print(y_train.shape)\n",
        "print(y_validate.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(600000, 2, 128)\n",
            "(600000, 2, 128)\n",
            "(600000, 2)\n",
            "(600000, 2)\n",
            "(570000, 2, 128)\n",
            "(30000, 2, 128)\n",
            "(570000, 2)\n",
            "(30000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPFcYnVZdC7u",
        "colab_type": "text"
      },
      "source": [
        "## Mapping Categorical labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSHyNNlNvye5",
        "colab_type": "code",
        "outputId": "f40447ab-11e5-4d70-cb72-c828637cdde4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "one_hot_encoder = preprocessing.OneHotEncoder()\n",
        "\n",
        "one_hot_encoder.fit(y_train[:, 0].reshape(-1,1))\n",
        "print(y_train[0])\n",
        "y_train = one_hot_encoder.transform(y_train[:, 0].reshape(-1,1)).todense()\n",
        "print(one_hot_encoder.inverse_transform(y_train[0]))\n",
        "\n",
        "one_hot_encoder.fit(y_validate[:, 0].reshape(-1,1))\n",
        "print(y_validate[0])\n",
        "y_validate = one_hot_encoder.transform(y_validate[:, 0].reshape(-1,1)).todense()\n",
        "print(one_hot_encoder.inverse_transform(y_validate[0]))\n",
        "\n",
        "one_hot_encoder.fit(y_test[:, 0].reshape(-1,1))\n",
        "print(y_test[0])\n",
        "y_test = one_hot_encoder.transform(y_test[:, 0].reshape(-1,1)).todense()\n",
        "print(one_hot_encoder.inverse_transform(y_test[0]))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[b'GFSK' b'-2']\n",
            "[[b'GFSK']]\n",
            "[b'BPSK' b'-4']\n",
            "[[b'BPSK']]\n",
            "[b'QAM16' b'-12']\n",
            "[[b'QAM16']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e1c6SZldaVA",
        "colab_type": "text"
      },
      "source": [
        "## Building NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbCFbvjZdHG7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Inputs to hidden layer linear transformation\n",
        "        self.hidden1 = nn.Linear(256, 400)\n",
        "        self.hidden2 = nn.Linear(400, 512)\n",
        "        #self.hidden3 = nn.Linear(768, 512)\n",
        "        #self.hidden4 = nn.Linear(512, 10)\n",
        "        # Output layer, 10 units - one for each mod type\n",
        "        self.output = nn.Linear(512, 10)\n",
        "        # Dropout module with 0.2 drop probability\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        #print(x.shape)\n",
        "        # Pass the input tensor through each of our operations\n",
        "        x = self.dropout(F.relu(self.hidden1(x)))\n",
        "        x = self.dropout(F.relu(self.hidden2(x)))\n",
        "        #x = self.dropout(F.relu(self.hidden3(x)))\n",
        "        #x = self.dropout(F.relu(self.hidden4(x)))\n",
        "        x = self.output(x)\n",
        "        \n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ9scZ-lMTzl",
        "colab_type": "text"
      },
      "source": [
        "# Converting  from numpy to tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16iQ8vgbMTZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = torch.from_numpy(x_train)\n",
        "y_train = torch.from_numpy(y_train)\n",
        "\n",
        "x_validate = torch.from_numpy(x_validate)\n",
        "y_validate  = torch.from_numpy(y_validate)\n",
        "\n",
        "x_test = torch.from_numpy(x_test)\n",
        "y_test = torch.from_numpy(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeG43e9FGC0S",
        "colab_type": "text"
      },
      "source": [
        "# Change dataset to loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wV3BOsIYGJxv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 2048\n",
        "num_workers = 0\n",
        "\n",
        "\n",
        "x_train_loader = torch.utils.data.DataLoader(x_train, batch_size=batch_size, shuffle=False)\n",
        "y_train_loader = torch.utils.data.DataLoader(y_train, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "x_validate_loader = torch.utils.data.DataLoader(x_validate, batch_size=batch_size, shuffle=False)\n",
        "y_validate_loader = torch.utils.data.DataLoader(y_validate, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "x_test_loader = torch.utils.data.DataLoader(x_test, batch_size=batch_size, shuffle=False)\n",
        "y_test_loader = torch.utils.data.DataLoader(y_test, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_aLue1wGMzi",
        "colab_type": "text"
      },
      "source": [
        "# Choose CPU or GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CRFkYn6GQXG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8MiigHNOzp2",
        "colab_type": "text"
      },
      "source": [
        "# Creating a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omntaAIbdYvy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Network()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "print(model)\n",
        "\n",
        "model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nk6ReScLNYvv",
        "colab_type": "text"
      },
      "source": [
        "# Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la02lfxhdieA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_epochs = 100\n",
        "steps = 0\n",
        "​\n",
        "batch_size = 2048\n",
        "num_workers = 0\n",
        "​\n",
        "y_train = y_train.long()\n",
        "y_validate = y_validate.long()\n",
        "y_test = y_test.long()\n",
        "​\n",
        "# divide data into batches \n",
        "x_train_loader = torch.utils.data.DataLoader(x_train, batch_size=batch_size, num_workers=num_workers)\n",
        "y_train_loader = torch.utils.data.DataLoader(y_train, batch_size=batch_size, num_workers=num_workers)\n",
        "​\n",
        "x_validate_loader = torch.utils.data.DataLoader(x_validate, batch_size=batch_size, num_workers=num_workers)\n",
        "y_validate_loader = torch.utils.data.DataLoader(y_validate, batch_size=batch_size, num_workers=num_workers)\n",
        "​\n",
        "x_test_loader = torch.utils.data.DataLoader(x_test, batch_size=batch_size, num_workers=num_workers)\n",
        "y_test_loader = torch.utils.data.DataLoader(x_test, batch_size=batch_size, num_workers=num_workers)\n",
        "​\n",
        "train_losses, validate_losses = [], []\n",
        "for e in range(n_epochs):\n",
        "    running_loss = 0\n",
        "    for samples, labels in zip(x_train_loader, y_train_loader):\n",
        "      \n",
        "        # Move input and label tensors to the default device\n",
        "        samples, labels = samples.to(device), labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        probabilities = model(samples)\n",
        "        # print(probabilities.dim())\n",
        "        # print(label.dim())\n",
        "        \n",
        "        loss = criterion(probabilities, torch.max(labels, 1)[1])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        \n",
        "    else:\n",
        "        validate_loss = 0\n",
        "        accuracy = 0\n",
        "        \n",
        "        # Turn off gradients for validation, saves memory and computations\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            for samples, labels in zip(x_validate_loader, y_validate_loader):\n",
        "              \n",
        "                # Move input and label tensors to the default device\n",
        "                samples, labels = samples.to(device), labels.to(device)\n",
        "                \n",
        "                \n",
        "                probabilities = model(samples)\n",
        "                validate_loss += criterion(probabilities, torch.max(labels, 1)[1])\n",
        "                \n",
        "                top_p, top_class = probabilities.topk(1, dim=1)\n",
        "                equals = top_class == torch.max(labels, 1)[1].view(*top_class.shape)\n",
        "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "        \n",
        "        model.train()\n",
        "        \n",
        "        train_losses.append(running_loss/len(y_train_loader))\n",
        "        validate_losses.append(validate_loss/len(y_test_loader))\n",
        "​\n",
        "        print(\"Epoch: {}/{}.. \".format(e+1, n_epochs),\n",
        "              \"Training Loss: {:.3f}.. \".format(train_losses[-1]),\n",
        "              \"Validation Loss: {:.3f}.. \".format(validate_losses[-1]),\n",
        "              \"Validation Accuracy: {:.3f}\".format(accuracy/len(y_validate_loader)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJF9BQJJE0TP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(train_losses, label='Training loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KxuPsh_f536",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(validate_losses, label='Validation loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XiW4LHuGkJr",
        "colab_type": "text"
      },
      "source": [
        "# Build CNN "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qKOBL8QCC4G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "      super().__init__()\n",
        "      # convolutional layer (sees 2x128x1 tensor)\n",
        "      self.conv1 = nn.Conv2d(1, 64, (1,3), padding=0)\n",
        "      # convolutional layer \n",
        "      self.conv2 = nn.Conv2d(64, 16, (2,3), padding=0)\n",
        "\n",
        "      self.fc1 = nn.Linear(3*128*16 , 128)\n",
        "      # linear layer (128 -> 10)\n",
        "      self.fc2 = nn.Linear(128, 10)\n",
        "      # dropout layer (p=0.25)\n",
        "      self.dropout = nn.Dropout(0.25)\n",
        "    \n",
        "  def forward(self, x):\n",
        "      pad1 = (1, 1)\n",
        "      pad2 = (1, 1, 1, 1)\n",
        "      x = F.pad(x, pad1, \"constant\", 0)\n",
        "      x = F.relu(self.conv1(x))\n",
        "      #print(x.shape)\n",
        "      x = F.pad(x, pad2, \"constant\", 0)\n",
        "      x = F.relu(self.conv2(x))\n",
        "      #print(x.shape)\n",
        "      x = x.view(x.shape[0], -1)\n",
        "      x = self.dropout(F.relu(self.fc1(x)))\n",
        "      x = self.fc2(x)\n",
        "     \n",
        "      return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb506ED0Gzqb",
        "colab_type": "text"
      },
      "source": [
        "# Creating CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbtGprSOFcuY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_CNN = CNN()\n",
        "optimizer = optim.Adam(model_CNN.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#move the model to device\n",
        "model_CNN.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4wOF_H4G_Mr",
        "colab_type": "text"
      },
      "source": [
        "# Train the CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OK20AssJG6QQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_epochs = 50\n",
        "steps = 0\n",
        "valid_loss_min = np.Inf # track change in validation loss\n",
        "\n",
        "train_losses, validate_losses = [], []\n",
        "for e in range(n_epochs):\n",
        "    running_loss = 0\n",
        "    for samples, labels in zip(x_train_loader, y_train_loader):\n",
        "      \n",
        "        # Move input and label tensors to the default device\n",
        "        samples, labels = samples.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        probabilities = model_CNN(torch.unsqueeze(samples, 1))  # add a depth size 1\n",
        "        \n",
        "        loss = criterion(probabilities, torch.max(labels, 1)[1])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        \n",
        "    else:\n",
        "        validate_loss = 0\n",
        "        accuracy = 0\n",
        "        correct = 0\n",
        "        # Turn off gradients for validation, saves memory and computations\n",
        "        with torch.no_grad():\n",
        "            model_CNN.eval()\n",
        "            for samples, labels in zip(x_validate_loader, y_validate_loader):\n",
        "              \n",
        "                # Move input and label tensors to the default device\n",
        "                samples, labels = samples.to(device), labels.to(device)\n",
        "                \n",
        "                \n",
        "                probabilities = model_CNN(torch.unsqueeze(samples, 1))  # add a depth size 1\n",
        "                validate_loss += criterion(probabilities, torch.max(labels, 1)[1])    \n",
        "                \n",
        "                top_p, top_class = probabilities.topk(1, dim=1)\n",
        "                equals = top_class == torch.max(labels, 1)[1].view(*top_class.shape)\n",
        "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "                correct += torch.sum(equals.type(torch.FloatTensor))\n",
        "        \n",
        "        model_CNN.train()\n",
        "        \n",
        "        train_losses.append(running_loss/len(y_train_loader))\n",
        "        validate_losses.append(validate_loss/len(y_validate_loader))\n",
        "        scheduler.step(validate_losses[-1])\n",
        "        \n",
        "        if validate_losses[-1] < valid_loss_min:\n",
        "          torch.save(model_CNN.state_dict(), 'model_CNN_RAW.pt')\n",
        "          print(\"saving model...\")\n",
        "          valid_loss_min = validate_losses[-1]\n",
        "\n",
        "        print(\"Epoch: {}/{}.. \".format(e+1, n_epochs),\n",
        "              \"Training Loss: {:.3f}.. \".format(train_losses[-1]),\n",
        "              \"Validation Loss: {:.3f}.. \".format(validate_losses[-1]),\n",
        "              \"Validation Accuracy: {:.3f}\".format(accuracy/len(y_validate_loader)))\n",
        "        #print(\"correct = \".correct)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M52AgPxkG6Fp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "batch_size = 2048\n",
        "num_workers = 0\n",
        "\n",
        "\n",
        "x_train = x_train.float()\n",
        "y_train = y_train.float()\n",
        "x_validate = x_validate.float()\n",
        "y_validate = y_validate.float()\n",
        "x_test = x_test.float()\n",
        "y_test = y_test.float()\n",
        "\n",
        "x_train_loader = torch.utils.data.DataLoader(x_train, batch_size=batch_size, shuffle=False)\n",
        "y_train_loader = torch.utils.data.DataLoader(y_train, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "x_validate_loader = torch.utils.data.DataLoader(x_validate, batch_size=batch_size, shuffle=False)\n",
        "y_validate_loader = torch.utils.data.DataLoader(y_validate, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "x_test_loader = torch.utils.data.DataLoader(x_test, batch_size=batch_size, shuffle=False)\n",
        "y_test_loader = torch.utils.data.DataLoader(y_test, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}